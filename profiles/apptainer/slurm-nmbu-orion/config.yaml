local-cores: 8 # front end / head node cores
cores: 128 # cluster cores
jobs: 128 # max parallel jobs
max-jobs-per-second: 3
max-status-checks-per-second: 3
keep-going: true 
#rerun-incomplete: true
latency-wait: 120
keep-incomplete: true
rerun-triggers: "mtime" # only schedule jobs to rerun if there has been a change in mtime of input/output files. That is: ignore code changes

# This one could be cool to have working 
show-failed-logs: false


# Apptainer
use-singularity: true

singularity-prefix: "~/.asscom2/singularity-prefix" # Directory in which singularity images will be stored. Couldn't find a way of using the $ASSCOM_BASE system variable, so users should manually change this here if they want to use something else.
singularity-args: "--bind $ASSCOM2_BASE,$ASSCOM2_DATABASES" # Must also bind ASSCOM2_BASE so we can access databases and report scripts etc. When this issue is solved I'll add individual arguments to rules and make it more flexible https://github.com/snakemake/snakemake/issues/262


# ---- Specific resources for Orion (Orion is a bit sluggish)

# Orion sometimes has some oversaturated nodes. This means that jobs submitted to this node will never finish. The only escape is by setting a higher core count which will decrease the computing density?
set-threads:
  - copy=3
  - sequence_lengths_individual=2


set-resources:
  - copy:mem_mb="1025"
  - copy:runtime="1h"



# ----  Specifics ----

cluster-cancel: "scancel"

cluster:
  mkdir -p results_ac2/logs/old &&
  sbatch
    --parsable
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --job-name=ac2_{rule}_{wildcards}
    --output=results_ac2/logs/%j-{rule}.out.log
    --error=results_ac2/logs/%j-{rule}.err.log
    --time={resources.runtime}
    --partition={resources.partition}

# Apparently, the account is not necessary on Orion.
# --account=YOURACCOUNTNAMEHERE

    

default-resources:
  - threads=2
  - mem_mb=1024
  - runtime="12h"
  - tmpdir="/work/users"
  - partition=smallmem,hugemem
  
