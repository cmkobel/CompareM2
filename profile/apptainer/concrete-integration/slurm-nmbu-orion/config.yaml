
local-cores: 8 # front end / head node cores
cores: 128 # cluster cores
jobs: 128 # max parallel jobs
max-jobs-per-second: 3
max-status-checks-per-second: 3
keep-going: true 
#rerun-incomplete: true
latency-wait: 120
keep-incomplete: false # true for debugging. Must be false for production, otherwise the user might get confusing --rerun-incomplete queries.
rerun-triggers: "mtime" # only schedule jobs to rerun if there has been a change in mtime of input/output files. That is: ignore code changes





# Apptainer
use-conda: true # This one is necessary to activate the environment inside the apptainer image.
use-singularity: true

# singularity-prefix: '~/.asscom2/singularity-prefix' # Directory in which singularity images will be stored. Couldn't find a way of using the $ASSCOM_BASE system variable, so users should manually change this here if they want to use something else.
# singularity-args: "--bind $ASSCOM2_BASE,$ASSCOM2_DATABASES" # Must also bind ASSCOM2_BASE so we can access databases and report scripts etc. When this issue is solved I'll add individual arguments to rules and make it more flexible https://github.com/snakemake/snakemake/issues/262
singularity-prefix: '/net/fs-2/scale/OrionStore/Courses/BIO326/PROK/ac26_apptainer_prefix'
singularity-args: '--bind "$ASSCOM2_BASE","$ASSCOM2_DATABASES","$(pwd)"' # Must also bind ASSCOM2_BASE so we can access databases and report scripts etc. When this issue is solved I'll add individual arguments to rules and make it more flexible https://github.com/snakemake/snakemake/issues/262




# ----  Specifics ----

cluster-cancel: "scancel"


cluster:
  sbatch
    --parsable
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --job-name=ac2_{rule}_{wildcards}
    --output=.snakemake/log/%j-{rule}.out.log
    --error=.snakemake/log/%j-{rule}.err.log
    --time={resources.runtime}
    --partition={resources.partition}

# Apparently, the account is not necessary on Orion.
# --account=YOURACCOUNTNAMEHERE

default-resources:
  - threads=2
  - mem_mb=1024
  - runtime="12h"
  - tmpdir="/work/users"
  - partition=smallmem,hugemem,hugemem-avx2,RStudio
  

# ---- Specific resources for Orion (Orion is a bit sluggish)

# Orion sometimes has some oversaturated nodes. This means that jobs submitted to this node will never finish. The only escape is by setting a higher core count which will decrease the computing density?
set-threads:
  - copy=3
  - sequence_lengths_individual=2


set-resources:
  - copy:mem_mb="1024"
  #- copy:runtime="60"
  #- copy:runtime="60m"

