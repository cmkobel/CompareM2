
local-cores: 8 # front end / head node cores
cores: 128 # cluster cores
jobs: 128 # max parallel jobs
max-jobs-per-second: 3
max-status-checks-per-second: 3
keep-going: true 
#rerun-incomplete: true
latency-wait: 120
keep-incomplete: false # true for debugging. Must be false for production, otherwise the user might get confusing --rerun-incomplete queries.
rerun-triggers: "mtime" # only schedule jobs to rerun if there has been a change in mtime of input/output files. That is: ignore code changes





# ---- Conda/Mamba ----
use-conda: true


conda-frontend: 'mamba'
conda-prefix: '~/.asscom2/conda'



# ----  Specifics ----

cluster-cancel: "scancel"


cluster:
  sbatch
    --parsable
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --job-name=ac2_{rule}_{wildcards}
    --output=.snakemake/log/%j-{rule}.out.log
    --error=.snakemake/log/%j-{rule}.err.log
    --time={resources.runtime}
    --partition={resources.partition}

# Apparently, the account is not necessary on Orion.
# --account=YOURACCOUNTNAMEHERE

default-resources:
  - threads=2
  - mem_mb=1024
  - runtime="12h"
  - tmpdir="/work/users"
  - partition=smallmem,hugemem
  

# ---- Specific resources for Orion (Orion is a bit sluggish)

# Orion sometimes has some oversaturated nodes. This means that jobs submitted to this node will never finish. The only escape is by setting a higher core count which will decrease the computing density?
set-threads:
  - copy=3
  - sequence_lengths_individual=2


set-resources:
  - copy:mem_mb="1025"
  #- copy:runtime="1h"
