# May the data passing through this pipeline
# somehow help to bring just a little more peace 
# in this troubled world.

__author__ = 'Carl M. Kobel'


# --- Development: Versioning ---------------------------------------
__version__ = "2.7.1"
# Places to bump
#  - here, because the snakefile can possibly be run without the ./asscom2 binary. Report gets from here.
#  - ./asscom2 binary
#  - changelog
# Also, paste changelog into the github release. Use pre-release and publish it after it has been tested.


# --- Development: Testing "nightly" --------------------------------

# For developing and testing (using conda) prior to publication of next version apptainer image, you can run the following from the repository directory:
# Conda
# export ASSCOM2_BASE="$(realpath ~/asscom2)"; export ASSCOM2_PROFILE="${ASSCOM2_BASE}/profile/conda/local"; ${ASSCOM2_BASE}/asscom2 --config input_genomes="${ASSCOM2_BASE}/tests/E._faecium/*.fna" --until fast

# If you haven't made any changes to the environment yamls, you can test using the apptainer/docker image.
# Apptainer
# export ASSCOM2_BASE="$(realpath ~/asscom2)"; export ASSCOM2_PROFILE="${ASSCOM2_BASE}/profile/apptainer/local"; ${ASSCOM2_BASE}/asscom2 --config input_genomes="${ASSCOM2_BASE}/tests/E._faecium/*.fna" --until fast


# --- Development: Update Dockerfile --------------------------------
# Update Dockerfile:
# export ASSCOM2_BASE="$(pwd -P)"; export ASSCOM2_PROFILE="profiles/apptainer/local"; snakemake --snakefile "${ASSCOM2_BASE}/snakefile" --configfile "${ASSCOM2_BASE}/config.yaml" --containerize > Dockerfile 
# And then remove the header text (asscom2 logo).

# --- Development: Update DAG figure for documentation --------------
# Update dag picture in documentation with this command (with anaconda/graphviz)
# asscom2 --forceall --rulegraph | dot -Tpdf > dag.pdf


import os
from os import listdir
from os.path import isfile, join
import pandas as pd
import numpy as np
from shutil import copyfile
import csv # for quoting 
import subprocess # For void_report
import datetime # For void_report

containerized: f"docker://cmkobel/assemblycomparator2:v{__version__}"
#containerized: f"docker://cmkobel/assemblycomparator2:latest" # DEBUG

# When executing, Snakemake will fail with a reasonable error message if the variables below are undefined.
envvars:
    "ASSCOM2_BASE",
    "ASSCOM2_PROFILE",
    "ASSCOM2_DATABASES",


# --- Functions -----------------------------------------------------

def get_input_genomes(pattern):
    """Runs ls on the pattern and parses what is returned. Uses module subprocess.
    """

    # Construct full command.
    # Using ls is the safest option, as the user will most likely be comfortable with how it globs.
    command = "ls -1 " + pattern

    # Run ls as a subprocess.
    ls = subprocess.run(command, shell = True, capture_output = True) # Apparently, shell = True is necessary when using advanced globbing symbols.

    # Parse and return
    decoded = ls.stdout.decode('utf-8').split("\n")
    decoded_nonempty = [i for i in decoded if i != ''] # If there are no matches, we must remove the empty result. Also, there is always an empty result in the end because ls returns a final newline.
    return decoded_nonempty
    
    
def interpret_true(text):
    return str(text).strip().lower() == "true"

# --- Field variables -----------------------------------------------

cwd = os.getcwd() # Could I fix #56 by making sure that the physical path of the cwd is obtained here? Update: Nope.
batch_title = cwd.split("/")[-1]
base_variable = os.environ['ASSCOM2_BASE'] # rename to ASSCOM2_BASE
DATABASES = os.environ['ASSCOM2_DATABASES'] # Defines where the databases are stored. One for all. when snakemake issue 262 is solved I'll make this more flexible for each rule.
results_directory = "results_ac2"
void_report = f"date -Iseconds >> {results_directory}/.asscom2_void_report.flag" # The modification time of this file tells the report subpipeline whether it needs to run. Thus, void_report is called in the end of every successful rule.




# --- Header --------------------------------------------------------

print("/*") # Makes it easy to export to .dot and to remove header from the generated Dockerfile.
print(f"                                                                   v{__version__}")
print("         █████╗ ███████╗███████╗ ██████╗ ██████╗ ███╗   ███╗██████╗ ")
print("        ██╔══██╗██╔════╝██╔════╝██╔════╝██╔═══██╗████╗ ████║╚════██╗")
print("        ███████║███████╗███████╗██║     ██║   ██║██╔████╔██║ █████╔╝")
print("        ██╔══██║╚════██║╚════██║██║     ██║   ██║██║╚██╔╝██║██╔═══╝ ")
print("        ██║  ██║███████║███████║╚██████╗╚██████╔╝██║ ╚═╝ ██║███████╗")
print("        ╚═╝  ╚═╝╚══════╝╚══════╝ ╚═════╝ ╚═════╝ ╚═╝     ╚═╝╚══════╝")
print("                       A.K.A. assemblycomparator2                   ")
print("                         Please log issues at:                      ")
print("              github.com/cmkobel/assemblycomparator2/issues         ")
print("                                                                    ")
print(f"    batch_title:           {batch_title}")
print(f"    base_variable:         {base_variable}")
print(f"    databases:             {DATABASES}")
print(f"    mlst_scheme:           {config['mlst_scheme']} (default automatic)")
print()
print("    Available rules:")
print("    sequence_lengths prokka dbcan interproscan busco")
print("    checkm2 diamond_kegg kegg_pathway panaroo snp_dists")
print("    assembly_stats gtdbtk abricate mlst mashtree fasttree")
print("    report downloads fast") # Todo: update.

    

# --- Parse input files -------------------------------------------------------


# Uses snakemakes built in config system to set default and custom parameters. 
input_genomes_parsed = get_input_genomes(config['input_genomes']) # Run with 'asscom2 --config input_genomes="my_files*.fna' to customize.

if len(input_genomes_parsed) < 1:
    raise Exception(f"Could not find {config['input_genomes']}. Quitting ...")



# --- Construct sample table ----------------------------------------

df = pd.DataFrame(data = {'input_file': input_genomes_parsed})

# Check that the directory is not empty.
if df.shape[0] == 0:
    raise Exception("Error: No fasta files in the current directory. Quitting ...")
    #raise Exception("Zero genomic files present.")
    sys.exit(1)

#df['sample_raw'] = [".".join(i.split(".")[:-1]) for i in df['input_file'].tolist()] # Extract everything before the extension dot.
df['basename'] = [os.path.basename(i) for i in df['input_file'].tolist()]

# Since mashtree doesn't like spaces in filenames, we must convert those.
df['basename_clean'] = df['basename'].str.replace(' ','_').str.replace(',','_').str.replace('"','_').str.replace('\'','_') # Convert punctuation marks to underscores: Makes everything easier.
df['sample'] = [".".join((i.split(".")[:-1])) for i in df['basename_clean']] # Remove extension by splitting, removing last, and joining.
df['extension'] =  [i.split(".")[-1] for i in df['input_file'].tolist()] # Extract extension
df['input_file_fasta'] = results_directory + "/samples/" + df['sample'] + "/" + df['sample'] + ".fna" # This is where the input file is copied to in the first snakemake rule.

df['1-index'] = [i+1 for i in range(len(df))]

# Check that the directory is not empty, again.
if df.shape[0] == 0:
    raise Exception("Error: No fasta files in the current directory. Quitting ...(2)")
    #raise Exception("Zero genomic files present.")
    sys.exit(1)


  
# --- Display sample table ------------------------------------------

print() # Visual padding
df = df.reset_index(drop = True)
#print(df[['input_file', 'sample', 'extension']])
#print(df[['input_file', 'extension', 'input_file_fasta']])
#print(df[['1-index', 'sample', 'extension']].to_string(index = False))
print(df[['1-index', 'input_file', 'basename', 'sample', 'extension', 'input_file_fasta']].to_string(index = False))
print("//")
print()

# Warn the user if there exists spaces in file names.
if any([" " in i for i in df['input_file'].tolist()]): # TODO test if it still works after new globbing system.
    print("Warning: One or more file names contain space(s). These have been replaced with underscores \" \" -> \"_\"")

# Check if the sample names are unique
duplicates = df[df.duplicated(['sample'])]
n_duplicates = duplicates.shape[0]
if n_duplicates > 0:
    raise Exception(f"Error: Sample names are not unique. The following ({n_duplicates}) input genome(s) are duplicated:\n{duplicates.to_string(index = False)}")


# The DATABASES directory must exist, otherwise apptainer gets confused and throws the following:
# WARNING: skipping mount of /home/thylakoid/assemblycomparator2/adatabaseas: stat /home/thylakoid/assemblycomparator2/adatabaseas: no such file or directory
if not os.path.isdir(DATABASES):
    os.mkdir(DATABASES)


# --- Make sure the output directory exists. ------------------------
if not os.path.isdir(results_directory):
    os.mkdir(results_directory) # If running with local profile, the directory won't be created. This is necessary in the edge case that the user _only_ runs "--until report".


localrules: metadata, annotate, checkm2_download, dbcan_download, busco_download, gtdb_download, report, install_report_environment_aot
#ruleorder: prokka > bakta > eggnog # I solved this by having an external output called ".annotation" that requests the appropriate annotator based on the config parameter "annotator".
#ruleorder: gapseq_find > gapseq # Most of the time, we just want the pathways completion fractions. Drafting and gapfilling a complete GEM is a bit overkill, but should of course be possible if the user wants it.

# --- Collect all targets -------------------------------------------
rule all:
    input: expand([\
        "{results_directory}/metadata.tsv", \
        "{results_directory}/.install_report_environment_aot.flag", \
        "{results_directory}/assembly-stats/assembly-stats.tsv", \
        "{results_directory}/samples/{sample}/sequence_lengths/{sample}_seqlen.tsv", \
        "{results_directory}/samples/{sample}/busco/short_summary_extract.tsv", \
        "{results_directory}/samples/{sample}/prokka/{sample}.gff", \
        "{results_directory}/samples/{sample}/bakta/{sample}.gff", \
        "{results_directory}/samples/{sample}/.annotation/{sample}.gff", \
        "{results_directory}/samples/{sample}/eggnog/{sample}.emapper.gff", \
        "{results_directory}/samples/{sample}/dbcan/overview.txt", \
        "{results_directory}/samples/{sample}/interproscan/{sample}_interproscan.tsv", \
        "{results_directory}/samples/{sample}/diamond_kegg/{sample}_diamond_kegg.tsv", \
        "{results_directory}/samples/{sample}/antismash/{sample}.json", \
        "{results_directory}/checkm2/quality_report.tsv", \
        "{results_directory}/kegg_pathway/kegg_pathway_enrichment_analysis.tsv", \
        "{results_directory}/gtdbtk/gtdbtk.summary.tsv", \
        "{results_directory}/abricate/card_detailed.tsv", \
        "{results_directory}/mlst/mlst.tsv", \
        "{results_directory}/panaroo/summary_statistics.txt", \
        "{results_directory}/snp-dists/snp-dists.tsv", \
        "{results_directory}/mashtree/mashtree.newick", \
        "{results_directory}/fasttree/fasttree.newick", \
        "{results_directory}/iqtree/core_genome_iqtree.treefile"], \
        results_directory = results_directory, sample = df["sample"]) 


        #"{results_directory}/samples/{sample}/gapseq/gapseq_done.flag", \

# Write the sample table for later reference.
# Why isn't this a run: instead of a shell: ?
rule metadata:
    input: df['input_file_fasta']
    output: "{results_directory}/metadata.tsv"
    params: dataframe = df.to_csv(None, index_label = "index", sep = "\t") #, quoting = csv.QUOTE_NONE)
    resources:
        runtime = "1h"
    #run:
        #df.to_csv(str(output), index_label = "index", sep = "\t")
        #os.system(f"cp ${{ASSCOM2_BASE}}/scripts/{report_template_file_basename} {results_directory}")
    shell: """

        echo '''{params.dataframe}''' > "{output:q}"

        {void_report}
    """


# --- Downloads -----------------------------------------------------

include: "rules/downloads.smk"


# --- Rules run per sample ------------------------------------------

# QC
include: "rules/sample_quality_control.smk"

# Annotation
include: "rules/sample_annotation.smk"

# Advanced annotation
include: "rules/sample_advanced_annotation.smk"



# --- Rules run per batch -------------------------------------------

# QC
include: "rules/batch_quality_control.smk"

# Advanced annotation
include: "rules/batch_advanced_annotation.smk"

# Clinical relevance
include: "rules/batch_clinical.smk"

# Core/pan 
include: "rules/batch_core_pan.smk"

# Phylogeny
include: "rules/batch_phylogeny.smk"



# --- Pro forma rules -----------------------------------------------

# This rule might seem silly, but it makes sure that the report environment is ready to rock when the report subpipeline eventually is run: This has two pros:
#    1) The vastly faster mamba configuration in the asscom2 pipeline is used
#    2) The conda/mamba debugging is taken care of, without having to wait for jobs to finish on fresh installations.
# Since all snakemake conda environments are installed in $SNAKEMAKE_CONDA_PREFIX set to ${ASSCOM2_BASE}/conda_base, reuse is guaranteed.
rule install_report_environment_aot:
    output: touch(f"{results_directory}/.install_report_environment_aot.flag")
    conda: "../dynamic_report/workflow/envs/r-markdown.yaml"
    shell: """

        echo "Report conda environment OK ..."

    """

# Just a dummy rule if you wanna force the report
# assemblycomparator2 --until report
# It isn't enough to just touch the file. The dynamic_report will not be triggered if the file is empty. Thus we add the date, and we have a nice debug log for seeing when the report was triggered.
# Will only but run if asked to. No need to use --forcerun, since snakemake states this in the output: "reason: Rules with neither input nor output files are always executed."
# Rule report does not depend on metadata, as the metadata is not interesting in itself.
rule report:
    shell: """
        
        {void_report}

    """



# --- Pseudo targets ------------------------------------------------

# Makes it easy to check that all databases are installed properly. Eventually for touching the database representatives in case of using prior installations.
# Max 6 databases. Can't take adding any more.
rule downloads:
    input:
        DATABASES + "/checkm2/ac2_checkm2_database_representative.flag",
        DATABASES + "/busco/ac2_busco_database_representative.flag",
        DATABASES + "/dbcan/ac2_dbcan_database_representative.flag",
        DATABASES + "/gtdb/ac2_gtdb_database_representative.flag",
        DATABASES + "/bakta/ac2_bakta_database_representative.flag",
        DATABASES + "/eggnog/ac2_eggnog_database_representative.flag",
        


# Blink-of-an-eye analysis
rule fast:
    input:
        expand(\
            ["{results_directory}/samples/{sample}/sequence_lengths/{sample}_seqlen.tsv", \
            "{results_directory}/assembly-stats/assembly-stats.tsv", \
            "{results_directory}/mashtree/mashtree.newick"], \
            results_directory = results_directory, \
            sample = df["sample"]) # TODO: define the expansion in each rule instead.


# Rules designed for bins of metagenomic origin
rule meta:
    input:
        expand(\
            ["{results_directory}/metadata.tsv", \
            "{results_directory}/.install_report_environment_aot.flag", \
            "{results_directory}/assembly-stats/assembly-stats.tsv", \
            "{results_directory}/samples/{sample}/sequence_lengths/{sample}_seqlen.tsv", \
            "{results_directory}/samples/{sample}/busco/short_summary_extract.tsv", \
            "{results_directory}/checkm2/quality_report.tsv", \
            "{results_directory}/samples/{sample}/diamond_kegg/{sample}_diamond_kegg.tsv", \
            "{results_directory}/kegg_pathway/kegg_pathway_enrichment_analysis.tsv", \
            "{results_directory}/samples/{sample}/dbcan/overview.txt", \
            "{results_directory}/samples/{sample}/interproscan/{sample}_interproscan.tsv", \
            "{results_directory}/gtdbtk/gtdbtk.summary.tsv", \
            "{results_directory}/mlst/mlst.tsv", \
            "{results_directory}/samples/{sample}/prokka/{sample}.gff", \
            "{results_directory}/mashtree/mashtree.newick"], \
            results_directory = results_directory, \
            sample = df["sample"])


# Rules designed for cultured isolates
rule isolate:
    input: expand(\
        ["{results_directory}/metadata.tsv", \
        "{results_directory}/.install_report_environment_aot.flag", \
        "{results_directory}/assembly-stats/assembly-stats.tsv", \
        "{results_directory}/samples/{sample}/sequence_lengths/{sample}_seqlen.tsv", \
        "{results_directory}/samples/{sample}/diamond_kegg/{sample}_diamond_kegg.tsv", \
        "{results_directory}/kegg_pathway/kegg_pathway_enrichment_analysis.tsv", \
        "{results_directory}/gtdbtk/gtdbtk.summary.tsv", \
        "{results_directory}/mlst/mlst.tsv", \
        "{results_directory}/abricate/card_detailed.tsv", \
        "{results_directory}/samples/{sample}/prokka/{sample}.gff", \
        "{results_directory}/panaroo/summary_statistics.txt", \
        "{results_directory}/fasttree/fasttree.newick", \
        "{results_directory}/snp-dists/snp-dists.tsv", \
        "{results_directory}/mashtree/mashtree.newick"], \
        results_directory = results_directory, sample = df["sample"])




# --- Dynamic report ------------------------------------------------
# For calling the report subpipeline we need some variables. The easiest way to communicate these from the main pipeline to the report pipeline, is to write a config.yaml.


onstart:
    print("Writing config for dynamic report pipeline")
    shell(f"""
    
        echo "# config for dynamic report pipeline" > .report_config.yaml
        echo batch_title: \"{batch_title}\" >> .report_config.yaml
        echo results_directory: \"{results_directory}\" >> .report_config.yaml
        echo base_variable: \"{base_variable}\" >> .report_config.yaml
        echo __version__: \"{__version__}\" >> .report_config.yaml
        
    """)
    
    
# TODO: A speedup could be to list the possible locations, and skipping the missing ones. By doing a full run and listing all possible files, these can be easily enlisted.
command_collect_version = f"""find results_ac2/ -name ".software_version.txt" | xargs cat | sort | uniq > {results_directory}/version_info.txt"""
onsuccess:
    shell(command_collect_version)
onerror:
    shell(command_collect_version)



print("*/") # Makes it easy to export to .dot and to remove header from the generated Dockerfile.

